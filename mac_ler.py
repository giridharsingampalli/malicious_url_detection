# -*- coding: utf-8 -*-
"""mac ler

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_6gmaGoA0XfFN9lkjxBupKH3CNL9FrX

Importing Libraries
"""

import pandas as pd
import itertools
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
from lightgbm import LGBMClassifier
import os
import seaborn as sns
from wordcloud import WordCloud

"""Loading Dataset"""

df=pd.read_csv('/content/malicious_phish.csv')

print(df.shape)
df.head()

df.type.value_counts()

"""Plotting Wordcloud"""

df_phish = df[df.type=='phishing']
df_malware = df[df.type=='malware']
df_deface = df[df.type=='defacement']
df_benign = df[df.type=='benign']

phish_url = " ".join(i for i in df_phish.url)
wordcloud = WordCloud(width=1600, height=800, colormap='Paired').generate(phish_url)
plt.figure( figsize=(12,14), facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

malware_url = " ".join(i for i in df_malware.url)
wordcloud = WordCloud(width=1600, height=800, colormap='Paired').generate(phish_url)
plt.figure( figsize=(12,14), facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

deface_url = " ".join(i for i in df_deface.url)
wordcloud = WordCloud(width=1600, height=800, colormap='Paired').generate(deface_url)
plt.figure( figsize=(12,14), facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

benign_url = " ".join(i for i in df_benign.url)
wordcloud = WordCloud(width=1600, height=800, colormap='Paired').generate(benign_url)
plt.figure( figsize=(12,14), facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""Feature Engineering"""

import re
#Use of IP or not in domain
def having_ip_address(url):
    match = re.search(
        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)' # IPv4 in hexadecimal
        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0
df['use_of_ip'] = df['url'].apply(lambda i: having_ip_address(i))

from urllib.parse import urlparse

def abnormal_url(url):
    hostname = urlparse(url).hostname
    hostname = str(hostname)
    match = re.search(hostname, url)
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0


df['abnormal_url'] = df['url'].apply(lambda i: abnormal_url(i))

#!pip install googlesearch-python

from googlesearch import search

def google_index(url):
    site = search(url, 5)
    return 1 if site else 0
df['google_index'] = df['url'].apply(lambda i: google_index(i))

def count_dot(url):
    count_dot = url.count('.')
    return count_dot

df['count.'] = df['url'].apply(lambda i: count_dot(i))
df.head()

def count_www(url):
    url.count('www')
    return url.count('www')

df['count-www'] = df['url'].apply(lambda i: count_www(i))

def count_atrate(url):

    return url.count('@')

df['count@'] = df['url'].apply(lambda i: count_atrate(i))


def no_of_dir(url):
    urldir = urlparse(url).path
    return urldir.count('/')

df['count_dir'] = df['url'].apply(lambda i: no_of_dir(i))

def no_of_embed(url):
    urldir = urlparse(url).path
    return urldir.count('//')

df['count_embed_domian'] = df['url'].apply(lambda i: no_of_embed(i))


def shortening_service(url):
    match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                      'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                      'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                      'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                      'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                      'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                      'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|'
                      'tr\.im|link\.zip\.net',
                      url)
    if match:
        return 1
    else:
        return 0

df['short_url'] = df['url'].apply(lambda i: shortening_service(i))

def count_https(url):
    return url.count('https')

df['count-https'] = df['url'].apply(lambda i : count_https(i))

def count_http(url):
    return url.count('http')

df['count-http'] = df['url'].apply(lambda i : count_http(i))

def count_per(url):
    return url.count('%')

df['count%'] = df['url'].apply(lambda i : count_per(i))

def count_ques(url):
    return url.count('?')

df['count?'] = df['url'].apply(lambda i: count_ques(i))

def count_hyphen(url):
    return url.count('-')

df['count-'] = df['url'].apply(lambda i: count_hyphen(i))

def count_equal(url):
    return url.count('=')

df['count='] = df['url'].apply(lambda i: count_equal(i))

def url_length(url):
    return len(str(url))


#Length of URL
df['url_length'] = df['url'].apply(lambda i: url_length(i))
#Hostname Length

def hostname_length(url):
    return len(urlparse(url).netloc)

df['hostname_length'] = df['url'].apply(lambda i: hostname_length(i))

df.head()

def suspicious_words(url):
    match = re.search('PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr',
                      url)
    if match:
        return 1
    else:
        return 0
df['sus_url'] = df['url'].apply(lambda i: suspicious_words(i))

def digit_count(url):
    digits = 0
    for i in url:
        if i.isnumeric():
            digits = digits + 1
    return digits

    #digit count feature
df['count-digits']= df['url'].apply(lambda i: digit_count(i))

def letter_count(url):
    letters = 0
    for i in url:
        if i.isalpha():
            letters = letters + 1
            return letters

    #letter count feature
df['count-letters']= df['url'].apply(lambda i: letter_count(i))

df.head()

#!pip install tld

#Importing dependencies
!pip install tld # Install the tld module
from urllib.parse import urlparse
from tld import get_tld
import os.path

#First Directory Length
def fd_length(url):
    urlpath= urlparse(url).path
    try:
        return len(urlpath.split('/')[1])
    except:
        return 0

df['fd_length'] = df['url'].apply(lambda i: fd_length(i))

#Length of Top Level Domain
df['tld'] = df['url'].apply(lambda i: get_tld(i,fail_silently=True))


def tld_length(tld):
    try:
        return len(tld)
    except:
        return -1

df['tld_length'] = df['tld'].apply(lambda i: tld_length(i))

df = df.drop("tld", axis = 1)

df.columns

df['type'].value_counts()

"""**EDA(EXPLOTRATORY DATA ANALYSIS)**

1.Distribution of use of ip
"""

import seaborn as sns
sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df ,hue="use_of_ip")

"""2.Distribution of use of abnormal url"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df ,hue="abnormal_url")

"""3.Distribution of use of Google index"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df ,hue="google_index")

"""4.Distribution of use of Short URL"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df ,hue="short_url")

"""5.Distribution of Suspicious URL"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df ,hue="sus_url")

"""6.Distribution of count of [.] dot"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count.", kind="box", data=df)

"""7.Distribution of count of WWW"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count-www", kind="box", data=df)

"""8.Distribution of count of @"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count@", kind="box", data=df)

"""9.Distribution of count_dir"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count_dir", kind="box", data=df)

"""10.Distribution of count of hostname length"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="hostname_length", kind="box", data=df)

"""11.Distribution of first directory length"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="fd_length", kind="box", data=df)

"""12.Distribution of top-level domain length"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="tld_length", kind="box", data=df)

"""Target Encoding"""

from sklearn.preprocessing import LabelEncoder

lb_make = LabelEncoder()
df["type_code"] = lb_make.fit_transform(df["type"])
df["type_code"].value_counts()

"""Segregating feature and Target variables"""

from sklearn.preprocessing import LabelEncoder

lb_make = LabelEncoder()
df["type_code"] = lb_make.fit_transform(df["type"])
df["type_code"].value_counts()
#Predictor Variables
# filtering out google_index as it has only 1 value
X = df[['use_of_ip','abnormal_url', 'count.', 'count-www', 'count@',
       'count_dir', 'count_embed_domian', 'short_url', 'count-https',
       'count-http', 'count%', 'count?', 'count-', 'count=', 'url_length',
       'hostname_length', 'sus_url', 'fd_length', 'tld_length', 'count-digits',
       'count-letters']]

#Target Variable
y = df['type_code']

X.head() # Change 'x' to 'X' to call the DataFrame assigned to 'X'

X.head()

X.columns

"""Train,Test and Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, shuffle=True, random_state=5)

"""MODEL BUILDING

Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics # Import the metrics module

rf = RandomForestClassifier(n_estimators=100,max_features='sqrt')
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
print(classification_report(y_test,y_pred_rf,target_names=['benign', 'defacement','phishing','malware']))
score = metrics.accuracy_score(y_test, y_pred_rf) # Now metrics is defined and can be used
print("accuracy:   %0.3f" % score)

"""rf.save('model.keras')"""

cm = confusion_matrix(y_test, y_pred_rf)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt='g')
plt.title('Confusion Matrix')
plt.ylabel('Actal Values')
plt.xlabel('Predicted Values')
plt.show()

feat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)
feat_importances.sort_values().plot(kind='barh',figsize=(10,6))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np

# Train the Random Forest model
rf = RandomForestClassifier(n_estimators=100, max_features='sqrt')
rf.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred_rf, target_names=['benign', 'defacement', 'phishing', 'malware']))

# Calculate accuracy
score = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy:   {score:.3f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_rf)

# Calculate specificity for each class
specificities = {}
for i, label in enumerate(['benign', 'defacement', 'phishing', 'malware']):
    # True Negatives (TN): All samples not belonging to the class (ignoring the row and column of the class)
    TN = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))

    # False Positives (FP): Samples not belonging to the class but predicted as that class
    FP = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]

    # Specificity for the class
    specificity = TN / (TN + FP)
    specificities[label] = specificity

# Print specificity for each class
for label, specificity in specificities.items():
    print(f"Specificity for {label}: {specificity:.3f}")

"""XGBoost"""

xgb_c = xgb.XGBClassifier(n_estimators= 100)
xgb_c.fit(X_train,y_train)
y_pred_x = xgb_c.predict(X_test)
print(classification_report(y_test,y_pred_x,target_names=['benign', 'defacement','phishing','malware']))
score = metrics.accuracy_score(y_test, y_pred_x)
print("accuracy:   %0.3f" % score)

cm = confusion_matrix(y_test, y_pred_rf)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt='.1f')
plt.title('Confusion Matrix')
plt.ylabel('Actal Values')
plt.xlabel('Predicted Values')
plt.show()

feat_importances = pd.Series(xgb_c.feature_importances_, index=X_train.columns)
feat_importances.sort_values().plot(kind='barh',figsize=(10,6))

import xgboost as xgb
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np

# Assuming you've already trained your model and made predictions
xgb_c = xgb.XGBClassifier(n_estimators=100)
xgb_c.fit(X_train, y_train)
y_pred_x = xgb_c.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred_x, target_names=['benign', 'defacement', 'phishing', 'malware']))

# Calculate accuracy
score = accuracy_score(y_test, y_pred_x)
print(f"accuracy:   {score:.3f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_x)

# Calculate specificity for each class
specificities = {}
for i, label in enumerate(['benign', 'defacement', 'phishing', 'malware']):
    # True Negatives: All samples not belonging to the class (ignoring the row and column of the class)
    TN = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))

    # False Positives: Samples not belonging to the class but predicted as that class
    FP = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]

    # Specificity for the class
    specificity = TN / (TN + FP)
    specificities[label] = specificity

# Print specificity for each class
for label, specificity in specificities.items():
    print(f"Specificity for {label}: {specificity:.3f}")

"""Light GBM Classifier"""


lgb = LGBMClassifier(objective='multiclass',boosting_type= 'gbdt',n_jobs = 5,
          silent = True, random_state=5)
LGB_C = lgb.fit(X_train, y_train)
y_pred_lgb = LGB_C.predict(X_test)
print(classification_report(y_test,y_pred_lgb,target_names=['benign', 'defacement','phishing','malware']))
score = metrics.accuracy_score(y_test, y_pred_lgb)
print("accuracy:   %0.3f" % score)

cm = confusion_matrix(y_test, y_pred_rf)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt='.1f')
plt.title('Confusion Matrix')
plt.ylabel('Actal Values')
plt.xlabel('Predicted Values')
plt.show()

feat_importances = pd.Series(lgb.feature_importances_, index=X_train.columns)
feat_importances.sort_values().plot(kind='barh',figsize=(10,6))

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
from lightgbm import LGBMClassifier

# Train the LGBM model
lgb = LGBMClassifier(objective='multiclass', boosting_type='gbdt', n_jobs=5,
                     silent=True, random_state=5)
LGB_C = lgb.fit(X_train, y_train)

# Make predictions on the test set
y_pred_lgb = LGB_C.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred_lgb, target_names=['benign', 'defacement', 'phishing', 'malware']))

# Calculate accuracy
score = accuracy_score(y_test, y_pred_lgb)
print(f"Accuracy:   {score:.3f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_lgb)

# Calculate specificity for each class
specificities = {}
for i, label in enumerate(['benign', 'defacement', 'phishing', 'malware']):
    # True Negatives (TN): All samples not belonging to the class (ignoring the row and column of the class)
    TN = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))

    # False Positives (FP): Samples not belonging to the class but predicted as that class
    FP = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]

    # Specificity for the class
    specificity = TN / (TN + FP)
    specificities[label] = specificity

# Print specificity for each class
for label, specificity in specificities.items():
    print(f"Specificity for {label}: {specificity:.3f}")

"""decision tree classifier"""

# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize the DecisionTreeClassifier
dt = DecisionTreeClassifier()

# Train the model
dt.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred_dt, target_names=['benign', 'defacement', 'phishing', 'malware']))

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred_dt)
print("Accuracy: {:.3f}".format(accuracy))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred_dt)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'defacement', 'phishing', 'malware'], yticklabels=['benign', 'defacement', 'phishing', 'malware'])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for Decision Tree Classifier')
plt.show()

# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

# Initialize the DecisionTreeClassifier
dt = DecisionTreeClassifier()

# Train the model
dt.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred_dt, target_names=['benign', 'defacement', 'phishing', 'malware']))

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred_dt)
print("Accuracy: {:.3f}".format(accuracy))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_dt)

# Calculate specificity for each class
specificities = {}
for i, label in enumerate(['benign', 'defacement', 'phishing', 'malware']):
    # True Negatives (TN): All samples not belonging to the class (ignoring the row and column of the class)
    TN = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))

    # False Positives (FP): Samples not belonging to the class but predicted as that class
    FP = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]

    # Specificity for the class
    specificity = TN / (TN + FP)
    specificities[label] = specificity

# Print specificity for each class
for label, specificity in specificities.items():
    print(f"Specificity for {label}: {specificity:.3f}")

"""Ada boost classifier

"""

!pip install scikit-learn
from sklearn.impute import SimpleImputer
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize SimpleImputer to replace NaN with the mean of the column
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on your training data and transform it
X_train_imputed = imputer.fit_transform(X_train)

# Initialize AdaBoost Classifier
ada = AdaBoostClassifier()

# Train the model using the imputed data
ada.fit(X_train_imputed, y_train)

# Impute missing values in X_test using the trained imputer
X_test_imputed = imputer.transform(X_test)

# Make predictions using the imputed test data
y_pred_ada = ada.predict(X_test_imputed)

# Print accuracy
accuracy = accuracy_score(y_test, y_pred_ada)
print("AdaBoost Accuracy: {:.3f}".format(accuracy))

cm = confusion_matrix(y_test, y_pred_ada)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'defacement', 'phishing', 'malware'], yticklabels=['benign', 'defacement', 'phishing', 'malware'])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for AdaBoost Classifier')
plt.show()

from sklearn.impute import SimpleImputer
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np

# Initialize SimpleImputer to replace NaN with the mean of the column
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on your training data and transform it
X_train_imputed = imputer.fit_transform(X_train)

# Initialize AdaBoost Classifier
ada = AdaBoostClassifier()

# Train the model using the imputed data
ada.fit(X_train_imputed, y_train)

# Impute missing values in X_test using the trained imputer
X_test_imputed = imputer.transform(X_test)

# Make predictions using the imputed test data
y_pred_ada = ada.predict(X_test_imputed)

# Print accuracy
accuracy = accuracy_score(y_test, y_pred_ada)
print("AdaBoost Accuracy: {:.3f}".format(accuracy))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_ada)

# Calculate specificity for each class
specificities = {}
for i, label in enumerate(['benign', 'defacement', 'phishing', 'malware']):
    # True Negatives (TN): All samples not belonging to the class (ignoring the row and column of the class)
    TN = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))

    # False Positives (FP): Samples not belonging to the class but predicted as that class
    FP = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]

    # Specificity for the class
    specificity = TN / (TN + FP)
    specificities[label] = specificity

# Print specificity for each class
for label, specificity in specificities.items():
    print(f"Specificity for {label}: {specificity:.3f}")

"""K nearest neighbors classifier"""

from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd

# Initialize SimpleImputer to replace NaN with the mean of the column
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on your training data and transform it
X_train_imputed = imputer.fit_transform(X_train)
# Transform the test data using the trained imputer
X_test_imputed = imputer.transform(X_test)

# Initialize K-Nearest Neighbors Classifier
knn = KNeighborsClassifier()

# Train the model using the imputed training data
knn.fit(X_train_imputed, y_train)

# Make predictions using the imputed test data
y_pred_knn = knn.predict(X_test_imputed)

# Print accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("K-Nearest Neighbors Accuracy: {:.3f}".format(accuracy))

cm = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'defacement', 'phishing', 'malware'], yticklabels=['benign', 'defacement', 'phishing', 'malware'])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for K-Nearest Neighbors Classifier')
plt.show()

from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np

# Initialize SimpleImputer to replace NaN with the mean of the column
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on your training data and transform it
X_train_imputed = imputer.fit_transform(X_train)

# Transform the test data using the trained imputer
X_test_imputed = imputer.transform(X_test)

# Initialize K-Nearest Neighbors Classifier
knn = KNeighborsClassifier()

# Train the model using the imputed training data
knn.fit(X_train_imputed, y_train)

# Make predictions using the imputed test data
y_pred_knn = knn.predict(X_test_imputed)

# Print accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("K-Nearest Neighbors Accuracy: {:.3f}".format(accuracy))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_knn)

# Calculate specificity for each class
specificities = {}
for i, label in enumerate(['benign', 'defacement', 'phishing', 'malware']):
    # True Negatives (TN): All samples not belonging to the class (ignoring the row and column of the class)
    TN = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))

    # False Positives (FP): Samples not belonging to the class but predicted as that class
    FP = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]

    # Specificity for the class
    specificity = TN / (TN + FP)
    specificities[label] = specificity

# Print specificity for each class
for label, specificity in specificities.items():
    print(f"Specificity for {label}: {specificity:.3f}")

"""extra tree classifier"""

from sklearn.impute import SimpleImputer
from sklearn.ensemble import ExtraTreesClassifier

# Initialize SimpleImputer to replace NaN with the mean of the column
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on your training data and transform it
X_train_imputed = imputer.fit_transform(X_train)
# Transform the test data using the trained imputer
X_test_imputed = imputer.transform(X_test)

# Initialize Extra Trees Classifier
extra_trees = ExtraTreesClassifier()

# Train the model using the imputed training data
extra_trees.fit(X_train_imputed, y_train) # Changed X_train to X_train_imputed

# Make predictions using the imputed test data
y_pred_extra_trees = extra_trees.predict(X_test_imputed) # Changed X_test to X_test_imputed

# Print accuracy
accuracy = accuracy_score(y_test, y_pred_extra_trees)
print("Extra Trees Classifier Accuracy: {:.3f}".format(accuracy))

cm = confusion_matrix(y_test, y_pred_extra_trees)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'defacement', 'phishing', 'malware'], yticklabels=['benign', 'defacement', 'phishing', 'malware'])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for Extra Trees Classifier')
plt.show()

from sklearn.metrics import confusion_matrix
import numpy as np

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_extra_trees)

# Initialize specificity list
specificities = []

# Loop over each class
for i in range(cm.shape[0]):
    # True Negatives (TN) for class i: all non-class i predictions (all rows excluding class i, and columns excluding class i)
    TN = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))
    # False Positives (FP) for class i: the number of times class i was predicted, but it's not class i
    FP = np.sum(cm[:, i]) - cm[i, i]

    # Specificity for class i
    specificity = TN / (TN + FP)
    specificities.append(specificity)

# Print specificities for each class
for i, specificity in enumerate(specificities):
    print(f"Specificity for class {i} ({['benign', 'defacement', 'phishing', 'malware'][i]}): {specificity:.3f}")

"""Prediction"""

def main(url):

    status = []

    status.append(having_ip_address(url))
    status.append(abnormal_url(url))
    status.append(count_dot(url))
    status.append(count_www(url))
    status.append(count_atrate(url))
    status.append(no_of_dir(url))
    status.append(no_of_embed(url))

    status.append(shortening_service(url))
    status.append(count_https(url))
    status.append(count_http(url))

    status.append(count_per(url))
    status.append(count_ques(url))
    status.append(count_hyphen(url))
    status.append(count_equal(url))

    status.append(url_length(url))
    status.append(hostname_length(url))
    status.append(suspicious_words(url))
    status.append(digit_count(url))
    status.append(letter_count(url))
    status.append(fd_length(url))
    tld = get_tld(url,fail_silently=True)

    status.append(tld_length(tld))

    return status

# predict function
def get_prediction_from_url(test_url):
    features_test = main(test_url)
    # Due to updates to scikit-learn, we now need a 2D array as a parameter to the predict function.
    features_test = np.array(features_test).reshape((1, -1))
    pred = lgb.predict(features_test)
    if int(pred[0]) == 0:

        res="SAFE"
        return res
    elif int(pred[0]) == 1.0:

        res="DEFACEMENT"
        return res
    elif int(pred[0]) == 2.0:
        res="PHISHING"
        return res

    elif int(pred[0]) == 3.0:

        res="MALWARE"
        return res


# predicting sample raw URLs

urls = ['titaniumcorporate.co.za','en.wikipedia.org/wiki/North_Dakota']

for url in urls:
     print(get_prediction_from_url(url))

rf.save('model.h5')
rf.save('model.keras')

from joblib import dump, load

# Save the model
dump(rf, 'random_forest_model.joblib')

# Load the model (when needed)
rf_loaded = load('random_forest_model.joblib')